[file_path]
; prefix of saving dir
prefix = .
; base dir, in which the dataset is saved
dataset_base_dir = generated_datasets
result_saving_base_dir = SDM_results

; Below items are for testing
; dir to save models
model_saving_dir = ${prefix}/${result_saving_base_dir}/${model:model_name}/models/2022-10-06_T04:27:32
; dir to save graphs, which will be plotted in model testing
test_results_saving_base_dir = ${prefix}/SDM_results

[dataset]
; dir name of your dataset, dataset_base_dir/dataset_name should be the absolute path of dataset
dataset_name = Camel/Daymet/RND/20
; sequence length, namely the num of past days
seq_len = 500
; num of input features
feature_num = 14
; batch size
batch_size = 16
; num of workers in dataloader
num_workers = 1
; evaluate every n steps
eval_every_n_steps = 60

[model]
; name of your model, will be the name of dir to save your models and logs
model_name = Camel_best/Daymet/RND/20
; whether concat input with missing mask
input_with_mask = True
; model type, Transformer/SAITS
model_type = Transformer
; num of layer groups
n_groups = 3
; num of group-inner layers
n_group_inner_layers = 1
; how to share parameters, inner_group/between_group
param_sharing_strategy = inner_group
; model hidden dim
d_model = 256
; hidden size of feed forward layer
d_inner = 2048
; head num of self-attention
n_head = 8
; key dim
d_k = 32
; value dim
d_v = 64
; drop out rate
dropout = 0
; whether to apply diagonal attention mask
diagonal_attention_mask = True
; cross sectional data in training
CS_data = True

[training]
; whether to have Masked Imputation Task (MIT) in training
MIT = True
; whether to have Observed Reconstruction Task (ORT) in training
ORT = True
; max num of training epochs
epochs = 10000
; which device for training, cpu/cuda
device = cuda:1
; learning rate
lr = 0.000669204734411692
; weight for reconstruction loss
reconstruction_loss_weight = 1
; weight for imputation loss
imputation_loss_weight = 1
; weight for COMP 11, 12 MSE loss
MSE_weight = 1
; patience of early stopping, -1 means not applied (current early stopping is based on total loss)
early_stop_patience = 15
; what type of optimizer to use, adam/adamw
optimizer_type = adam
; weight decay used in optimizer
weight_decay = 0
; max_norm for gradient clipping, set 0 to disable
max_norm = 0
; strategy on model saving, all/best/none. If set as none, then do not save models (mode for hyper-parameter searching)
model_saving_strategy = best
; training data imputation strategy, random or time series missing
MIT_RND = True
; training data artificial_missing_rate
artificial_missing_rate = 0.2

[test]
; whether to save imputed data
save_imputations = True
; name of model your select for testing
step_184 = model_trainStep_20340_valStep_339_imputationMAE_0.0905
; absolute path to locate model you select
model_path = ${file_path:model_saving_dir}/${step_184}
; path of dir to save generated figs (PR-curve etc.)
result_saving_path = ${file_path:test_results_saving_base_dir}/${model:model_name}/step_184